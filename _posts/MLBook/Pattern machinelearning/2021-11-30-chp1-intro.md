---
title : "Intro,polynominal curve"



excerpt: "Pattern Recognition and Machine Learning"

categories:
  - PatternRecognitionML
tags:
  - [Machine Learning,Pattern Recognition and Machine Learning,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---


pattern recognition and machine learning이라는 책으로 high-level 관점에서의 저의 deep learning 지식을  좀 더 넓게 그리고 low-level에서 바라보려는 시도를 하려고 합니다. 이 블로그 포스트는 책을 공부하고 복습하고 덮고 글로 작성한 것입니다. 놓친 부분은 책을 보고 추가해서 정리합니다



## digit classifier

![image](https://user-images.githubusercontent.com/50165842/143983371-50241a75-bec9-4ddf-b45a-9c30d829bbb7.png)

이 책은 처음에 손글씨 digit classifier의 예제로 시작을 합니다.   28 X 28의 hand-written-digit이 있습니다. 이 digit들은 $${X_1, X_2 ,... X_n}$$ 라고 표기할 수 있습니다. 이 digit들은 ***training*** 단계를 거치고 test set과 ***비교***를 합니다. 그러면서  *** generalization*** 됩니다. 이 generalization이 training의 목적입니다.

### preprocess

이 input data 들은 training 하기 전에 pre-process라는 transform 단계를 거치게 됩니다. 예를 들어, 이 digit의 경우 pre-process를 통해 한 scale의 feature들을 고정된 크기로 바꿔줍니다. digit의 ***<u>위치와 크기의 다양성이 줄어들게 되므로</u>*** pattern recognition 알고리즘이 ***<u>다른 클래스의 digit을  비교적 쉽게 분류</u>*** 할 수 있게 됩니다.  이는  feature selection이라 불리기도 합니다. 주의할 점은 test data도 train data와 마찬가지로 pre-process를 해줘야합니다.

pre-process는 또한 ***<u>computation speed</u>***를 증가시켜줍니다. 예를 들어, 고해상도의 비디오 스트림에서 얼굴을 찾는 시나리오가 있습니다.   스트림 이미지들을 가공하지 않고 바로 detection 알고리즘을 적용하게 되면, 상당한 computing power가 요구될 것입니다. 대신에, 얼굴의 위치를 찾는 것이 목적이므로, 얼굴인지 얼굴이 아닌지를 구별하는데 유용한 정보들을 찾습니다. (ex. pixel의 그룹을 average 값으로 처리, 일종의 dimension reduction). 찾은  feature들을 detection 알고리즘에 적용하게 되면 비교적 빠르게 얼굴을 식별할 수 있습니다. 



하지만, 만약 유용하지 않다고 여긴 정보에서 중요한 정보가 있었다면, 성능의 하락을 일으킬 것입니다.

## machine learning

여기서는 pattern recognition의 종류를 소개하듯이 나왔지만, 저는 machine learning의 종류로 생각하고 적도록 하겠습니다.

### supervised learning

지도학습은 ***<u>data가 주어지고 label(desired solution)이 주어 졌을 때</u>*** 푸는 문제를 말합니다. ***<u>finite number of discrete categorical value</u>***를 예측하는 문제를 ***<u>classification</u>***, ***<u>한 개 혹은 여러 개의 continuous value</u>***를 예측하는 문제를 ***<u>regression</u>***이라고 합니다.

### unsupervised learning

비지도 학습은 ***<u>data가 target value(label) 없이 주어 졌을 때</u>***의 학습 방법입니다. 비지도 학습은 data가 주어지면 이 ***<u>data들을 유사도에 따라 분류</u>***할 수 있습니다. 이를, ***<u>clustering</u>***이라고 합니다. data가 주어지면 ***<u>이 data의 분포를 결정</u>***할 수 있습니다. 이를, ***<u>density estimation</u>***이라고 합니다***<u>. high-dimension의 data를 two dimension이나 three dimension으로 projection</u>*** 하여 ***<u>visualization</u>***을 할 수도 있습니다.

### reinforcment learning

environment, agent가 있고, ***<u>agent가 주어진 상황(environment)에 따라서 action</u>***을 하고 ***<u>그에 따라 얻게 되는 reward를 maximize</u>*** 하는 문제를 의미합니다. ***<u>이 알고리즘은 supervised learning과는 다르게 optimal solution이 주어지지 않는다는 것입니다. trial and error를 통하여 optimal solution을 찾아 나가게 됩니다.</u>*** 

#### exploitation vs exploration

***<u>강화학습은 exploitation과 exploration의 trade-off</u>***입니다. ***<u>exploration은 새로운 것을 action 해서 얼마나 효과적인지 알려고 하는 것</u>***으로 생각하면 됩니다. ***<u>exploitation은 제일 높은 보상을 준다고 알려진 action을 하려는 것</u>***으로 생각하면 됩니다. 둘 중 하나에만 치우치면 좋은 결과가 나오지 않습니다.



## 앞으로 다루게 될것

이 책은 앞으로 이러한 단순한 예제(digit)들을 좀 더 복잡한 모델의 관점에서 보게 될 것이라고 합니다. 또한, 중요한 tool들인 information theory, probability theory, decision theory를 다루게 될 것이라고 합니다.

## example:polynomial curve

![image](https://user-images.githubusercontent.com/50165842/143988734-c695e506-c1e7-4d57-baf4-b8ed4ceae873.png)

우선은 간단한 regression 문제로 시작을 해보도록 하겠습니다. synthetic generated data를 예측하는 모델입니다. 우리가 이 data를 생성하는 과정을 알고 있기에 우리가 학습할 모델이랑 비교할 수 있어서 선택했다고 합니다. 그냥, $$\sin(2\pi x)$$ 를 predict 하는 모델로 봐도 됩니다.



### suppose

N개의 observation sample이 있다고 가정합니다. N개의 관측치 x가 이처럼 정의됩니다. $$ X \equiv (X_1,X_2,....X_N)^T $$ . 이에 따라오는 target value t는 다음과 같이 정의합니다. $$ t \equiv (t_1,t_2,....t_N)^T$$ 

### goal

model을 다음과 같이 정의하겠습니다. $$y(x,w) = w_0 + w_1x + w_2x^2 + .... w_MX^M = \sum_{j=1}^M w_jx^j$$

우리의 목표는 주어진 training data로 모델을 학습시켜서 앞으로 들어올 $$\hat{x}$$ 에 대한 $$\hat{t}$$ 를 예측하는 것입니다.

위 모델은 앞으로 Chapter 3, 4에 게 다루게 될 linear model입니다.

### Error function

위에 대한 error function을 정의해야합니다. 

$$ E(w) = 1/2 \sum_{j=1}^N (y(x_j,w) - t_j)$$ 



![image](https://user-images.githubusercontent.com/50165842/143988471-5e8faa7f-5cdc-4aaa-a746-735e4a5a40f5.png)

이 sum-of-squares error의 기하학적인 표현은 위의 그림과 같습니다.

우리의 문제를 풀기 위해서는 $$E(w)$$ 를 최소화 하는 해 $$w*$$ 를 찾는 것입니다. 따라서, 최종 예측되는 모델은 $$y(x,w^*)$$ 가 될 것입니다. 

### M=1,2...,9

![image](https://user-images.githubusercontent.com/50165842/143988698-c5ca10d9-4c84-4ce7-81aa-60211ac051e6.png)

M=3 일 때 가장 well-fit한 모델처럼 보입니다. M=9일 때에도 모든 data point를 통과하는 모델이 생성 됩니다. 즉, M=9일 때는 $$E(w^*) = 0$$ 이 됩니다.  하지만,  data point 사이간의 model의 oscillation이 심합니다.  이를 overfitting이라고 합니다. 

가끔씩은 , data의 size가 다를 때 이를 동일한 scale 혹은 기준에서 비교하기 위해 RMSE를 사용하는 것도 좋은 선택지가 될 수 있습니다.

$$RMSE = \sqrt{E(w_*)/N} $$ 로 정의됩니다. 

![image](https://user-images.githubusercontent.com/50165842/143989557-df2bdbe8-aa17-42a7-9551-41c4c8ad07d6.png)



Rmse를 plot을 해보면 M= 9인 시점부터 test error 가 급격하게 증가함을 알 수 있습니다. 

![image](https://user-images.githubusercontent.com/50165842/143989717-ba0095d1-e6d5-4b1a-b80b-1f7c6e801482.png)

위 table로 부터 M이 증가함에 따라 $$w*$$ 의 절대값이 점점 커짐을 알 수 있습니다.

### sampe size and model complexity

![image](https://user-images.githubusercontent.com/50165842/143990027-59ce76b4-ed06-4d2d-8c98-070d32da9e0d.png)

여기서 흥미로운 사실이 하나 발견됩니다. data의 size가 변함에 따라 oscillation이 줄어 든다는 것입니다. 하지만, 그렇다고 매번 data size가 달라짐에 따라 모델의 complexity를 조율하는 것은 비효율 적입니다. 모델의 complexity를 조절하는 방법중 하나로는 parameter 수를 줄이는 방법이 있습니다. model의 parameter수가 model complexity를 결정하는데 필수적인 요소는 아닙니다. 이는 Chp 3에서 자세하게 다룬다고 합니다.

저렇게 하기 보다는 , bayesian approcah 나 maximum likelihood 를 사용해서 data를 표현하는데 필요한 parameter 수를 찾을 수 있습니다.



하지만, 우리는 여기서 다른 방법으로 이를 해결할 것입니다.

모델의 complexity를 줄이기 위해서 regualrization 기법을 사용할 수 있습니다. 그 중에서 , l2 reguarlization을 사용해보도록 하겠습니다.

error 를 다시 정의하게 됩니다.  

$$E(w^*) + {\lambda \over 2 }\begin{Vmatrix} w \end{Vmatrix} ^{2}  $$

$$ \begin{Vmatrix} w \end{Vmatrix} ^{2}  =   w^Tw = w_0^2 + w_1^2 + .. w_M^2 $$

종종, 계수 $$w_0$$는 정규화 식에서 생략되어집니다. 왜냐하면, 이를 포함시키게 되면 결과값이 target의 범위에 따라 달라지기 때문이라고 합니다.(잘 모르겟습니다.) . 혹은 포함될 수 있지만, 자체 정규화 계수가 있다고합니다.(이는 chp 5.5.1 에서 자세히 다룰 예정이라고 합니다.) 

![image](https://user-images.githubusercontent.com/50165842/143991571-4cbb0eb8-7bb2-4f6f-95ac-36ea5f805d62.png)

위의 그래프는 람다 값에 따른 model의 complexity를 보여줍니다.

![image](https://user-images.githubusercontent.com/50165842/143991815-51ba1606-8941-4f1a-8a32-8be057d5f87f.png)

위 테이블은 람다 값에 따라 coefficient $$w^*$$ 의 절대값의 범위가 달라짐을 보여줍니다. 

## 마무리

이 complexity issue는 나중에 좀 더 chp1.3에서 자세하게 다룬다고 합니다. 지금은 단순하게 처리를 하였지만 , 상황에 따라 적합한 방법을 찾아서 대처할 줄 알아야합니다.  위 경우는 , train data를 train ,valid(hold-out) set으로 나누어서 complexity를 측정했다고 볼 수 있습니다.